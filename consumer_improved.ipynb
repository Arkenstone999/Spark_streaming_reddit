{"cells": [{"cell_type": "markdown", "id": "06c23d58", "metadata": {}, "source": ["# Reddit Spark Streaming Consumer\n", "This notebook receives Reddit posts/comments from a socket, stores them to a Spark table, and computes metrics such as reference counts, TF-IDF top words, and sentiment analysis."]}, {"cell_type": "code", "execution_count": null, "id": "19848401", "metadata": {}, "outputs": [], "source": ["import json\n", "import re\n", "from textblob import TextBlob\n", "from pyspark.sql import SparkSession\n", "from pyspark.sql import functions as F\n", "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n", "from pyspark.sql.functions import udf\n", "from pyspark.sql.types import DoubleType"]}, {"cell_type": "markdown", "id": "7e84db33", "metadata": {}, "source": ["### Set-up of Spark Streaam Consumer and Data Schema structure.\n", "##### See command to initialize spark server inside code cell. "]}, {"cell_type": "code", "execution_count": null, "id": "91442269", "metadata": {}, "outputs": [], "source": ["spark = SparkSession.builder.appName('RedditConsumer').getOrCreate()\n", "\n", "# Command to run spark server on docker to plug into kernel for running notebook\n", "# docker run -it -p 4040:4040 -p 8080:8080 -p 8081:8081 -p 8888:8888 -p 5432:5432 --cpus=2 --memory=2048m -h spark -w /mnt/host_home/ pyspark_container jupyter-lab --ip 0.0.0.0 --port 8888 --no-browser --allow-root\n", "\n", "HOST = 'host.docker.internal'\n", "PORT = 9998\n", "\n", "schema = StructType([\n", "    StructField('type', StringType()),\n", "    StructField('subreddit', StringType()),\n", "    StructField('id', StringType()),\n", "    StructField('text', StringType()),\n", "    StructField('created_utc', DoubleType()),\n", "    StructField('author', StringType())\n", "])\n", "\n", "raw_lines = (spark\n", "    .readStream\n", "    .format('socket')\n", "    .option('host', HOST)\n", "    .option('port', PORT)\n", "    .load())\n", "\n", "json_df = raw_lines.select(F.from_json(F.col('value'), schema).alias('data')).select('data.*')"]}, {"cell_type": "markdown", "id": "26de7808", "metadata": {}, "source": ["### Write data to memory"]}, {"cell_type": "code", "execution_count": null, "id": "b5e80f21", "metadata": {}, "outputs": [], "source": ["query_memory = (json_df\n", "    .writeStream\n", "    .outputMode('append')\n", "    .format('memory')\n", "    .queryName('raw')\n", "    .start())"]}, {"cell_type": "markdown", "id": "4750c995", "metadata": {}, "source": ["### Get reference to users, subreddits and URLs"]}, {"cell_type": "code", "execution_count": null, "id": "2c5ac289", "metadata": {}, "outputs": [], "source": ["# Get reference to users, subreddits, and URLs in the text by using regex\n", "user_refs = F.regexp_extract_all(\"text\", r\"/u/[^\\s]+\")\n", "subreddit_refs = F.regexp_extract_all(\"text\", r\"/r/[^\\s]+\")\n", "url_refs = F.regexp_extract_all(\"text\", r\"https?://[^\\s]+\")"]}, {"cell_type": "markdown", "id": "84340fed", "metadata": {}, "source": ["### Create dataframes of references on a sliding window basis."]}, {"cell_type": "code", "execution_count": null, "id": "c722b809", "metadata": {}, "outputs": [], "source": ["# get the count of each type of reference and tag them with a created timestamp\n", "# for time based filtering and aggregation\n", "refs_df = json_df.select(\n", "    F.col('created_utc').cast('timestamp').alias('created_ts'),\n", "    F.size(user_refs).alias('user_ref_count'),\n", "    F.size(subreddit_refs).alias('subreddit_ref_count'),\n", "    F.size(url_refs).alias('url_ref_count')\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "23664300", "metadata": {}, "outputs": [], "source": ["# get the total references per time window (60 seconds with a 5 second slide)\n", "windowed_refs = (refs_df\n", "    .withWatermark('created_ts', '1 minute')\n", "    .groupBy(F.window('created_ts', '60 seconds', '5 seconds'))\n", "    .sum('user_ref_count', 'subreddit_ref_count', 'url_ref_count')\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "4875096b", "metadata": {}, "outputs": [], "source": ["# Write the windowed reference counts to an in-memory table and to Parquet files\n", "ref_query = (windowed_refs\n", "    .writeStream\n", "    .outputMode('update')\n", "    .format('console')\n", "    .option('truncate', False)\n", "    .start())"]}, {"cell_type": "markdown", "id": "1339e11b", "metadata": {}, "source": ["### Function to compute TF-IDF and find top 10 most important words in the window."]}, {"cell_type": "code", "execution_count": null, "id": "b7e5df20", "metadata": {}, "outputs": [], "source": ["from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n", "\n", "def compute_tfidf():\n", "    raw_df = spark.sql('select * from raw')\n", "    tokenizer = Tokenizer(inputCol='text', outputCol='words')\n", "    words_data = tokenizer.transform(raw_df)\n", "    remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n", "    filtered = remover.transform(words_data)\n", "    hashingTF = HashingTF(inputCol='filtered', outputCol='rawFeatures', numFeatures=10000)\n", "    featurized = hashingTF.transform(filtered)\n", "    idf = IDF(inputCol='rawFeatures', outputCol='features')\n", "    idf_model = idf.fit(featurized)\n", "    tfidf = idf_model.transform(featurized)\n", "    zipped = tfidf.select(F.explode(F.arrays_zip('filtered', 'features')).alias('z'))\n", "    word_scores = zipped.select(F.col('z.filtered').alias('word'), F.col('z.features').alias('score'))\n", "    top_words = word_scores.groupBy('word').agg(F.max('score').alias('score')).orderBy(F.desc('score')).limit(10)\n", "    top_words.show(truncate=False)\n"]}, {"cell_type": "markdown", "id": "5236f7a1", "metadata": {}, "source": ["### TextBlob function to achieve sentiment analysis of text."]}, {"cell_type": "code", "execution_count": null, "id": "5da49b3a", "metadata": {}, "outputs": [], "source": ["@udf(returnType=DoubleType())\n", "def sentiment_udf(text):\n", "    return TextBlob(text).sentiment.polarity if text else 0.0"]}, {"cell_type": "markdown", "id": "eab952be", "metadata": {}, "source": ["#### Batch Processing of Streaming Data.\n", "- TODO:\n", "    - Requires references in window created previously\n", "    - Requires top 10 words in TF-IDF\n", "    - Write data to processed memory"]}, {"cell_type": "code", "execution_count": null, "id": "c7266310", "metadata": {}, "outputs": [], "source": ["def process_batch(batch_df):\n", "    batch_df.cache()\n", "    batch_df.createOrReplaceTempView('current_batch')\n", "    count = batch_df.count()\n", "    print(f'Processing batch with {count} records')\n", "\n", "    refs = (batch_df.select(\n", "                F.regexp_extract_all('text', r'/u/\\w+').alias('users'),\n", "                F.regexp_extract_all('text', r'/r/\\w+').alias('subs'),\n", "                F.regexp_extract_all('text', r'https?://[^\\s]+').alias('urls'))\n", "            .select(\n", "                F.size('users').alias('user_refs'),\n", "                F.size('subs').alias('sub_refs'),\n", "                F.size('urls').alias('url_refs')))\n", "    refs_summary = refs.groupBy().sum('user_refs', 'sub_refs', 'url_refs')\n", "    refs_summary.show(truncate=False)\n", "    refs_summary.createOrReplaceTempView('batch_references')\n", "\n", "    tokenizer = Tokenizer(inputCol='text', outputCol='words')\n", "    words = tokenizer.transform(batch_df)\n", "    remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n", "    filtered = remover.transform(words)\n", "    hashingTF = HashingTF(inputCol='filtered', outputCol='rawFeatures', numFeatures=10000)\n", "    featurized = hashingTF.transform(filtered)\n", "    idf = IDF(inputCol='rawFeatures', outputCol='features')\n", "    idf_model = idf.fit(featurized)\n", "    tfidf = idf_model.transform(featurized)\n", "\n", "    zipped = tfidf.select(F.explode(F.arrays_zip('filtered','features')).alias('z'))\n", "    word_scores = zipped.select(F.col('z.filtered').alias('word'), F.col('z.features').alias('score'))\n", "    top_words = word_scores.groupBy('word').agg(F.max('score').alias('score')).orderBy(F.desc('score')).limit(10)\n", "    top_words.show(truncate=False)\n", "    top_words.createOrReplaceTempView('batch_tfidf')\n"]}, {"cell_type": "code", "execution_count": null, "id": "18628f8b", "metadata": {}, "outputs": [], "source": ["process_query = (json_df\n", "    .writeStream\n", "    .foreachBatch(process_batch)\n", "    .start())"]}, {"cell_type": "markdown", "id": "315f5ce9", "metadata": {}, "source": ["### Terminating all streams."]}, {"cell_type": "code", "execution_count": null, "id": "a226e048", "metadata": {}, "outputs": [], "source": ["# Await termination of all streams\n", "for q in [query_memory, ref_query, process_query]:\n", "    q.awaitTermination()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3", "version": "3.10.9"}}, "nbformat": 4, "nbformat_minor": 5}
