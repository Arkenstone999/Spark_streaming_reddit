{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Reddit Spark Streaming Consumer\nThis notebook receives Reddit posts/comments from a socket, stores them to a Spark table, and computes metrics such as reference counts, TF-IDF top words, and sentiment analysis."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import json\nimport re\nfrom textblob import TextBlob\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType\n\nspark = SparkSession.builder.appName('RedditConsumer').getOrCreate()\n\nHOST = 'host.docker.internal'  # change if running locally\nPORT = 9998\n\nschema = StructType([\n    StructField('type', StringType()),\n    StructField('subreddit', StringType()),\n    StructField('id', StringType()),\n    StructField('text', StringType()),\n    StructField('created_utc', DoubleType()),\n    StructField('author', StringType())\n])\n\nraw_lines = (spark\n    .readStream\n    .format('socket')\n    .option('host', HOST)\n    .option('port', PORT)\n    .load())\n\njson_df = raw_lines.select(F.from_json(F.col('value'), schema).alias('data')).select('data.*')\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Write incoming data to an in-memory table and to Parquet files\nquery_memory = (json_df\n    .writeStream\n    .outputMode('append')\n    .format('memory')\n    .queryName('raw')\n    .start())\n\nquery_files = (json_df\n    .writeStream\n    .outputMode('append')\n    .format('parquet')\n    .option('path', 'data/raw')\n    .option('checkpointLocation', 'chk/raw')\n    .start())\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Helper expressions for counting references\nuser_refs = F.expr(\"regexp_extract_all(text, '/u/[^\\s]+')\")\nsubreddit_refs = F.expr(\"regexp_extract_all(text, '/r/[^\\s]+')\")\nurl_refs = F.expr(\"regexp_extract_all(text, 'https?://[^\\s]+')\")\n\nrefs_df = json_df.select(\n    F.col('created_utc').cast('timestamp').alias('created_ts'),\n    F.size(user_refs).alias('user_ref_count'),\n    F.size(subreddit_refs).alias('subreddit_ref_count'),\n    F.size(url_refs).alias('url_ref_count')\n)\n\nwindowed_refs = (refs_df\n    .withWatermark('created_ts', '1 minute')\n    .groupBy(F.window('created_ts', '60 seconds', '5 seconds'))\n    .sum('user_ref_count', 'subreddit_ref_count', 'url_ref_count')\n)\n\nref_query = (windowed_refs\n    .writeStream\n    .outputMode('update')\n    .format('console')\n    .option('truncate', False)\n    .start())\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# TF-IDF top words based on history stored in table 'raw'\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n\ndef compute_tfidf():\n    raw_df = spark.sql('select * from raw')\n    tokenizer = Tokenizer(inputCol='text', outputCol='words')\n    words_data = tokenizer.transform(raw_df)\n    remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n    filtered = remover.transform(words_data)\n    hashingTF = HashingTF(inputCol='filtered', outputCol='rawFeatures', numFeatures=10000)\n    featurized = hashingTF.transform(filtered)\n    idf = IDF(inputCol='rawFeatures', outputCol='features')\n    idf_model = idf.fit(featurized)\n    tfidf = idf_model.transform(featurized)\n    zipped = tfidf.select(F.explode(F.arrays_zip('filtered', 'features')).alias('z'))\n    word_scores = zipped.select(F.col('z.filtered').alias('word'), F.col('z.features').alias('score'))\n    top_words = word_scores.groupBy('word').agg(F.max('score').alias('score')).orderBy(F.desc('score')).limit(10)\n    top_words.show(truncate=False)\n\ncompute_tfidf()\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Time range and sentiment analysis for each batch\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType\n\n@udf(returnType=DoubleType())\ndef sentiment_udf(text):\n    return TextBlob(text).sentiment.polarity if text else 0.0\n\ndef process_batch(batch_df, epoch_id):\n    batch_df.cache()\n    batch_df.createOrReplaceTempView('raw')\n    time_bounds = spark.sql('SELECT MIN(created_utc) as min_ts, MAX(created_utc) as max_ts FROM raw')\n    time_bounds.show()\n    sentiments = batch_df.withColumn('sentiment', sentiment_udf('text'))\n    avg_sent = sentiments.agg(F.avg('sentiment')).collect()[0][0]\n    print(f'Average sentiment: {avg_sent}')\n\nprocess_query = (json_df\n    .writeStream\n    .foreachBatch(process_batch)\n    .start())\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Await termination of all streams\nfor q in [query_memory, query_files, ref_query, process_query]:\n    q.awaitTermination()\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}
