{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Spark Streaming Consumer\n",
    "This notebook receives Reddit posts/comments from a socket, stores them to a Spark table, and computes metrics such as reference counts, TF-IDF top words, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.appName('RedditConsumer').getOrCreate()\n",
    "\n",
    "HOST = 'host.docker.internal'  # change if running locally\n",
    "PORT = 9998\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('type', StringType()),\n",
    "    StructField('subreddit', StringType()),\n",
    "    StructField('id', StringType()),\n",
    "    StructField('text', StringType()),\n",
    "    StructField('created_utc', DoubleType()),\n",
    "    StructField('author', StringType())\n",
    "])\n",
    "\n",
    "raw_lines = (spark\n",
    "    .readStream\n",
    "    .format('socket')\n",
    "    .option('host', HOST)\n",
    "    .option('port', PORT)\n",
    "    .load())\n",
    "\n",
    "json_df = raw_lines.select(F.from_json(F.col('value'), schema).alias('data')).select('data.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write incoming data to an in-memory table and to Parquet files\n",
    "query_memory = (json_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('memory')\n",
    "    .queryName('raw')\n",
    "    .start())\n",
    "\n",
    "query_files = (json_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('parquet')\n",
    "    .option('path', 'data/raw')\n",
    "    .option('checkpointLocation', 'chk/raw')\n",
    "    .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper expressions for counting references\n",
    "user_refs = F.expr(\"regexp_extract_all(text, '/u/[^\\s]+')\")\n",
    "subreddit_refs = F.expr(\"regexp_extract_all(text, '/r/[^\\s]+')\")\n",
    "url_refs = F.expr(\"regexp_extract_all(text, 'https?://[^\\s]+')\")\n",
    "\n",
    "#user_refs = F.regexp_extract_all(\"text\", r\"/u/[^\\s]+\")\n",
    "#subreddit_refs = F.regexp_extract_all(\"text\", r\"/r/[^\\s]+\")\n",
    "#url_refs = F.regexp_extract_all(\"text\", r\"https?://[^\\s]+\")\n",
    "\n",
    "\n",
    "refs_df = json_df.select(\n",
    "    F.col('created_utc').cast('timestamp').alias('created_ts'),\n",
    "    F.size(user_refs).alias('user_ref_count'),\n",
    "    F.size(subreddit_refs).alias('subreddit_ref_count'),\n",
    "    F.size(url_refs).alias('url_ref_count')\n",
    ")\n",
    "\n",
    "windowed_refs = (refs_df\n",
    "    .withWatermark('created_ts', '1 minute')\n",
    "    .groupBy(F.window('created_ts', '60 seconds', '5 seconds'))\n",
    "    .sum('user_ref_count', 'subreddit_ref_count', 'url_ref_count')\n",
    ")\n",
    "\n",
    "ref_query = (windowed_refs\n",
    "    .writeStream\n",
    "    .outputMode('update')\n",
    "    .format('console')\n",
    "    .option('truncate', False)\n",
    "    .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF top words based on history stored in table 'raw'\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "def compute_tfidf():\n",
    "    raw_df = spark.sql('select * from raw')\n",
    "    tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "    words_data = tokenizer.transform(raw_df)\n",
    "    remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n",
    "    filtered = remover.transform(words_data)\n",
    "    hashingTF = HashingTF(inputCol='filtered', outputCol='rawFeatures', numFeatures=10000)\n",
    "    featurized = hashingTF.transform(filtered)\n",
    "    idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "    idf_model = idf.fit(featurized)\n",
    "    tfidf = idf_model.transform(featurized)\n",
    "    zipped = tfidf.select(F.explode(F.arrays_zip('filtered', 'features')).alias('z'))\n",
    "    word_scores = zipped.select(F.col('z.filtered').alias('word'), F.col('z.features').alias('score'))\n",
    "    top_words = word_scores.groupBy('word').agg(F.max('score').alias('score')).orderBy(F.desc('score')).limit(10)\n",
    "    top_words.show(truncate=False)\n",
    "\n",
    "compute_tfidf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time range and sentiment analysis for each batch\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def sentiment_udf(text):\n",
    "    return TextBlob(text).sentiment.polarity if text else 0.0\n",
    "\n",
    "def process_batch(batch_df):\n",
    "    batch_df.cache()\n",
    "    batch_df.createOrReplaceTempView('raw')\n",
    "    time_bounds = spark.sql('SELECT MIN(created_utc) as min_ts, MAX(created_utc) as max_ts FROM raw')\n",
    "    time_bounds.show()\n",
    "    sentiments = batch_df.withColumn('sentiment', sentiment_udf('text'))\n",
    "    avg_sent = sentiments.agg(F.avg('sentiment')).collect()[0][0]\n",
    "    print(f'Average sentiment: {avg_sent}')\n",
    "\n",
    "process_query = (json_df\n",
    "    .writeStream\n",
    "    .foreachBatch(process_batch)\n",
    "    .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Await termination of all streams\n",
    "for q in [query_memory, query_files, ref_query, process_query]:\n",
    "    q.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
