{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c23d58",
   "metadata": {},
   "source": [
    "# Reddit Spark Streaming Consumer\n",
    "This notebook receives Reddit posts/comments from a socket, stores them to a Spark table, and computes metrics such as reference counts, TF-IDF top words, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ca7c798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/bitnami/python/lib/python3.11/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /opt/bitnami/python/lib/python3.11/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /opt/bitnami/python/lib/python3.11/site-packages (from nltk>=3.9->textblob) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/bitnami/python/lib/python3.11/site-packages (from nltk>=3.9->textblob) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/bitnami/python/lib/python3.11/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/bitnami/python/lib/python3.11/site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19848401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84db33",
   "metadata": {},
   "source": [
    "### Set-up of Spark Streaam Consumer and Data Schema structure.\n",
    "##### See command to initialize spark server inside code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91442269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 10:37:31 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('RedditConsumer').getOrCreate()\n",
    "\n",
    "# Command to run spark server on docker to plug into kernel for running notebook\n",
    "# docker run -it -p 4040:4040 -p 8080:8080 -p 8081:8081 -p 8888:8888 -p 5432:5432 --cpus=2 --memory=2048m -h spark -w /mnt/host_home/ pyspark_container jupyter-lab --ip 0.0.0.0 --port 8888 --no-browser --allow-root\n",
    "\n",
    "HOST = 'host.docker.internal'\n",
    "PORT = 9998\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('type', StringType()),\n",
    "    StructField('subreddit', StringType()),\n",
    "    StructField('id', StringType()),\n",
    "    StructField('text', StringType()),\n",
    "    StructField('created_utc', DoubleType()),\n",
    "    StructField('author', StringType())\n",
    "])\n",
    "\n",
    "raw_lines = (spark\n",
    "    .readStream\n",
    "    .format('socket')\n",
    "    .option('host', HOST)\n",
    "    .option('port', PORT)\n",
    "    .load())\n",
    "\n",
    "json_df = raw_lines.select(F.from_json(F.col('value'), schema).alias('data')).select('data.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de7808",
   "metadata": {},
   "source": [
    "### Write data to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e80f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 10:37:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7b68e808-f0f3-4f46-8728-861dd54a4c13. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/08 10:37:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name raw as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m query_memory = (\u001b[43mjson_df\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriteStream\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mappend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmemory\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mraw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/streaming/readwriter.py:1527\u001b[39m, in \u001b[36mDataStreamWriter.start\u001b[39m\u001b[34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[39m\n\u001b[32m   1525\u001b[39m     \u001b[38;5;28mself\u001b[39m.queryName(queryName)\n\u001b[32m   1526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1528\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sq(\u001b[38;5;28mself\u001b[39m._jwrite.start(path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: Cannot start query with name raw as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "source": [
    "query_memory = (json_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('memory')\n",
    "    .queryName('raw')\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750c995",
   "metadata": {},
   "source": [
    "### Get reference to users, subreddits and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5ac289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reference to users, subreddits, and URLs in the text by using regex\n",
    "user_refs = F.regexp_extract_all(\"text\", r'/u/[^\\s]+')\n",
    "subreddit_refs = F.regexp_extract_all(\"text\", r'/r/[^\\s]+')\n",
    "url_refs = F.regexp_extract_all(\"text\", r'https?://[^\\s]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84340fed",
   "metadata": {},
   "source": [
    "### Create dataframes of references on a sliding window basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c722b809",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `/u/[^\\s]+` cannot be resolved. Did you mean one of the following? [`author`, `subreddit`, `id`, `text`, `type`].;\n'Project [cast(created_utc#8 as timestamp) AS created_ts#81, size(regexp_extract_all(text#7, '/u/[^\\s]+, 1), true) AS user_ref_count#82, size(regexp_extract_all(text#7, '/r/[^\\s]+, 1), true) AS subreddit_ref_count#83, size(regexp_extract_all(text#7, 'https?://[^\\s]+, 1), true) AS url_ref_count#84]\n+- Project [data#2.type AS type#4, data#2.subreddit AS subreddit#5, data#2.id AS id#6, data#2.text AS text#7, data#2.created_utc AS created_utc#8, data#2.author AS author#9]\n   +- Project [from_json(StructField(type,StringType,true), StructField(subreddit,StringType,true), StructField(id,StringType,true), StructField(text,StringType,true), StructField(created_utc,DoubleType,true), StructField(author,StringType,true), value#0, Some(Etc/UTC)) AS data#2]\n      +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.TextSocketSourceProvider@4b501810, socket, org.apache.spark.sql.execution.streaming.sources.TextSocketTable@6619c4a9, [host=host.docker.internal, port=9998], [value#0]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# get the count of each type of reference and tag them with a created timestamp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# for time based filtering and aggregation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m refs_df = \u001b[43mjson_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcreated_utc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcreated_ts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_refs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser_ref_count\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubreddit_refs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msubreddit_ref_count\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_refs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43murl_ref_count\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/dataframe.py:3223\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m   3178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   3179\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   3180\u001b[39m \n\u001b[32m   3181\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3221\u001b[39m \u001b[33;03m    +-----+---+\u001b[39;00m\n\u001b[32m   3222\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3223\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `/u/[^\\s]+` cannot be resolved. Did you mean one of the following? [`author`, `subreddit`, `id`, `text`, `type`].;\n'Project [cast(created_utc#8 as timestamp) AS created_ts#81, size(regexp_extract_all(text#7, '/u/[^\\s]+, 1), true) AS user_ref_count#82, size(regexp_extract_all(text#7, '/r/[^\\s]+, 1), true) AS subreddit_ref_count#83, size(regexp_extract_all(text#7, 'https?://[^\\s]+, 1), true) AS url_ref_count#84]\n+- Project [data#2.type AS type#4, data#2.subreddit AS subreddit#5, data#2.id AS id#6, data#2.text AS text#7, data#2.created_utc AS created_utc#8, data#2.author AS author#9]\n   +- Project [from_json(StructField(type,StringType,true), StructField(subreddit,StringType,true), StructField(id,StringType,true), StructField(text,StringType,true), StructField(created_utc,DoubleType,true), StructField(author,StringType,true), value#0, Some(Etc/UTC)) AS data#2]\n      +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.TextSocketSourceProvider@4b501810, socket, org.apache.spark.sql.execution.streaming.sources.TextSocketTable@6619c4a9, [host=host.docker.internal, port=9998], [value#0]\n"
     ]
    }
   ],
   "source": [
    "# get the count of each type of reference and tag them with a created timestamp\n",
    "# for time based filtering and aggregation\n",
    "refs_df = json_df.select(\n",
    "    F.col('created_utc').cast('timestamp').alias('created_ts'),\n",
    "    F.size(user_refs).alias('user_ref_count'),\n",
    "    F.size(subreddit_refs).alias('subreddit_ref_count'),\n",
    "    F.size(url_refs).alias('url_ref_count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23664300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total references per time window (60 seconds with a 5 second slide)\n",
    "windowed_refs = (refs_df\n",
    "    .withWatermark('created_ts', '1 minute')\n",
    "    .groupBy(F.window('created_ts', '60 seconds', '5 seconds'))\n",
    "    .sum('user_ref_count', 'subreddit_ref_count', 'url_ref_count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the windowed reference counts to an in-memory table and to Parquet files\n",
    "ref_query = (windowed_refs\n",
    "    .writeStream\n",
    "    .outputMode('update')\n",
    "    .format('console')\n",
    "    .option('truncate', False)\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a22e9",
   "metadata": {},
   "source": [
    "### Function to extract references to users, subreddits and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c78fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reference_counts(batch_df):\n",
    "    refs = (batch_df.select(\n",
    "                F.regexp_extract_all('text', r'/u/\\w+').alias('users'),\n",
    "                F.regexp_extract_all('text', r'/r/\\w+').alias('subs'),\n",
    "                F.regexp_extract_all('text', r'https?://[^\\s]+').alias('urls'))\n",
    "            .select(\n",
    "                F.size('users').alias('user_refs'),\n",
    "                F.size('subs').alias('sub_refs'),\n",
    "                F.size('urls').alias('url_refs')))\n",
    "    refs_summary = refs.groupBy().sum('user_refs', 'sub_refs', 'url_refs')\n",
    "    return refs_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1339e11b",
   "metadata": {},
   "source": [
    "### Function to compute TF-IDF and find top 10 most important words in the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "def compute_tfidf(batch_df):\n",
    "    tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "    words = tokenizer.transform(batch_df)\n",
    "    remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n",
    "    filtered = remover.transform(words)\n",
    "    hashingTF = HashingTF(inputCol='filtered', outputCol='rawFeatures', numFeatures=10000)\n",
    "    featurized = hashingTF.transform(filtered)\n",
    "    idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "    idf_model = idf.fit(featurized)\n",
    "    tfidf = idf_model.transform(featurized)\n",
    "\n",
    "    zipped = tfidf.select(F.explode(F.arrays_zip('filtered','features')).alias('z'))\n",
    "    word_scores = zipped.select(F.col('z.filtered').alias('word'), F.col('z.features').alias('score'))\n",
    "    top_words = word_scores.groupBy('word').agg(F.max('score').alias('score')).orderBy(F.desc('score')).limit(10)\n",
    "    \n",
    "    return top_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236f7a1",
   "metadata": {},
   "source": [
    "### TextBlob function to achieve sentiment analysis of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da49b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=DoubleType())\n",
    "def sentiment_udf(text):\n",
    "    return TextBlob(text).sentiment.polarity if text else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab952be",
   "metadata": {},
   "source": [
    "#### Batch Processing of Streaming Data.\n",
    "- TODO:\n",
    "    - Requires references in window created previously\n",
    "    - Requires top 10 words in TF-IDF\n",
    "    - Write data to processed memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7266310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df):\n",
    "    batch_df.cache()\n",
    "    batch_df.createOrReplaceTempView('current_batch')\n",
    "    count = batch_df.count()\n",
    "    print(f'Processing batch with {count} records')\n",
    "\n",
    "    refs_summary = extract_reference_counts(batch_df)\n",
    "    refs_summary.show(truncate=False)\n",
    "    refs_summary.createOrReplaceTempView('batch_references')\n",
    "\n",
    "    top_words = compute_tfidf(batch_df)\n",
    "    top_words.show(truncate=False)\n",
    "    top_words.createOrReplaceTempView('batch_tfidf')\n",
    "\n",
    "    sentiment_scores = batch_df.withColumn('sentiment', sentiment_udf(F.col('text')))\n",
    "    sentiment_scores.createOrReplaceTempView('batch_sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18628f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_query = (json_df\n",
    "    .writeStream\n",
    "    .foreachBatch(process_batch)\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb18156",
   "metadata": {},
   "source": [
    "### Kill Spark Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b099247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f5ce9",
   "metadata": {},
   "source": [
    "### Terminating all streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226e048",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ref_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Await termination of all streams\u001b[39;00m\n\u001b[32m      2\u001b[39m spark.stop()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m [query_memory, \u001b[43mref_query\u001b[49m, process_query]:\n\u001b[32m      4\u001b[39m     q.awaitTermination()\n",
      "\u001b[31mNameError\u001b[39m: name 'ref_query' is not defined"
     ]
    }
   ],
   "source": [
    "# Await termination of all streams\n",
    "for q in [query_memory, ref_query, process_query]:\n",
    "    q.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c0f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
