{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Reddit Spark Streaming Consumer"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import json\nfrom textblob import TextBlob\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "HOST = 'host.docker.internal'\nPORT = 9998\nspark = SparkSession.builder.appName('RedditConsumer').getOrCreate()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "schema = StructType([\n    StructField('type', StringType()),\n    StructField('subreddit', StringType()),\n    StructField('id', StringType()),\n    StructField('text', StringType()),\n    StructField('created_utc', DoubleType()),\n    StructField('author', StringType())\n])"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "raw_lines = (spark.readStream.format('socket').option('host', HOST).option('port', PORT).load())\njson_df = raw_lines.select(F.from_json(F.col('value'), schema).alias('data')).select('data.*')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "query_memory = (json_df.writeStream.outputMode('append').format('memory').queryName('raw').start())\nquery_files = (json_df.writeStream.outputMode('append').format('parquet').option('path', 'data/raw').option('checkpointLocation', 'chk/raw').start())"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "user_refs = F.expr(\"regexp_extract_all(text, '/u/[^\\\\s]+')\")\nsub_refs = F.expr(\"regexp_extract_all(text, '/r/[^\\\\s]+')\")\nurl_refs = F.expr(\"regexp_extract_all(text, 'https?://[^\\\\s]+')\")\nrefs_df = json_df.select(F.col('created_utc').cast('timestamp').alias('created_ts'), F.size(user_refs).alias('user_ref_count'), F.size(sub_refs).alias('sub_ref_count'), F.size(url_refs).alias('url_ref_count'))\nwindowed_refs = (refs_df.withWatermark('created_ts', '1 minute').groupBy(F.window('created_ts', '60 seconds', '5 seconds')).sum('user_ref_count', 'sub_ref_count', 'url_ref_count'))\nref_query = (windowed_refs.writeStream.outputMode('update').format('console').option('truncate', False).start())"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n@F.udf('double')\ndef sentiment_udf(text):\n    return float(TextBlob(text).sentiment.polarity) if text else 0.0\n\ndef compute_tfidf(df):\n    tokenizer = Tokenizer(inputCol='text', outputCol='words')\n    words = tokenizer.transform(df)\n    remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n    filtered = remover.transform(words)\n    hashingTF = HashingTF(inputCol='filtered', outputCol='rawFeatures', numFeatures=10000)\n    featurized = hashingTF.transform(filtered)\n    idf = IDF(inputCol='rawFeatures', outputCol='features')\n    idf_model = idf.fit(featurized)\n    tfidf = idf_model.transform(featurized)\n    zipped = tfidf.select(F.explode(F.arrays_zip('filtered', 'features')).alias('z'))\n    scores = zipped.select(F.col('z.filtered').alias('word'), F.col('z.features').alias('score'))\n    top_words = scores.groupBy('word').agg(F.max('score').alias('score')).orderBy(F.desc('score')).limit(10)\n    top_words.show(truncate=False)\n\ndef process_batch(batch_df, epoch_id):\n    batch_df.persist()\n    if spark.catalog.tableExists('raw'):\n        full_df = spark.table('raw').unionByName(batch_df)\n    else:\n        full_df = batch_df\n    full_df.createOrReplaceTempView('raw')\n    batch_df.write.mode('append').parquet('data/raw')\n    bounds = full_df.agg(F.min('created_utc').alias('min_ts'), F.max('created_utc').alias('max_ts')).collect()[0]\n    print(f\"Data time range: {bounds['min_ts']} - {bounds['max_ts']}\")\n    sentiments = batch_df.withColumn('sentiment', sentiment_udf('text'))\n    avg_sent = sentiments.agg(F.avg('sentiment')).collect()[0][0]\n    print(f'Average sentiment (batch): {avg_sent}')\n    top_authors = batch_df.groupBy('author').count().orderBy(F.desc('count')).limit(5)\n    top_authors.show(truncate=False)\n    compute_tfidf(full_df)\n    batch_df.unpersist()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "process_query = json_df.writeStream.foreachBatch(process_batch).start()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "for q in [query_memory, query_files, ref_query, process_query]:\n    q.awaitTermination()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
